# CPU Optimization Strategies for RAG

## Performance Without GPU

### Current Setup (Baseline)
- **Model**: all-MiniLM-L6-v2 (384 dimensions)
- **Device**: CPU
- **Speed**: ~10ms per document
- **Quality**: Excellent for most use cases

---

## Optimization Options (Ranked by Speed Gain)

### 1. Ô∏èüöÄ **ONNX Runtime** (2-4x faster)
**Best for**: Production deployments

```python
from src.config.optimized_embedder import ONNXEmbedder

embedder = ONNXEmbedder("all-MiniLM-L6-v2", batch_size=64)
embeddings = await embedder.embed_texts(texts)
```

**Benefits:**
- 2-4x faster CPU inference
- 30-50% lower memory usage
- Same quality as PyTorch
- Microsoft-backed optimization

**Speed**: ~3-5ms per document ‚úÖ

**Install:**
```bash
pip install optimum[onnxruntime] onnxruntime
```

---

### 2. ‚ö° **Quantized Models** (2-3x faster, 4x smaller)
**Best for**: Limited RAM scenarios

```python
from src.config.optimized_embedder import QuantizedEmbedder

embedder = QuantizedEmbedder("all-MiniLM-L6-v2")
embeddings = await embedder.embed_texts(texts)
```

**Benefits:**
- 4x smaller model size (14MB ‚Üí 3.5MB)
- 2-3x faster inference
- <2% quality loss
- Lower memory footprint

**Speed**: ~4-6ms per document ‚úÖ

---

### 3. üîÑ **Multi-Process** (Linear scaling with cores)
**Best for**: Large batch processing (1000+ documents)

```python
from src.config.optimized_embedder import MultiProcessEmbedder

embedder = MultiProcessEmbedder(
    "all-MiniLM-L6-v2",
    num_workers=4  # Use all CPU cores
)
embeddings = await embedder.embed_texts(large_batch)
```

**Benefits:**
- Scales with CPU cores (4 cores = 4x faster)
- Best for batch processing
- No quality loss

**Speed**: ~2.5ms per document (4 cores) ‚úÖ

---

### 4. üì¶ **Smaller Models** (3-5x faster)
**Best for**: Real-time applications

**Options:**

| Model | Size | Speed | Dimensions | Quality |
|-------|------|-------|------------|---------|
| `all-MiniLM-L6-v2` | 14MB | Fast | 384 | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `all-MiniLM-L12-v2` | 33MB | Medium | 384 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `all-mpnet-base-v2` | 420MB | Slow | 768 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

```python
# Fastest option
embedder = SentenceTransformerEmbedder(
    EmbedderConfig(model_name="all-MiniLM-L6-v2")
)
```

---

## Alternative Libraries (No GPU Needed)

### 1. **Jina AI API** (Cloud-based, fastest)
**Speed**: ~50ms per request (network latency)
**Cost**: Free tier: 1M tokens/month

```python
# Already had this before refactoring
# Can switch back if needed
```

**Pros:**
- No local compute needed
- State-of-art models (jina-embeddings-v3: 1024 dim)
- Handles load spikes

**Cons:**
- Network latency
- API costs at scale
- Privacy concerns (data leaves server)

---

### 2. **Cohere Embed** (Cloud API)
**Speed**: ~30ms per request
**Cost**: Free tier: 100 requests/min

```python
pip install cohere

import cohere
co = cohere.Client("YOUR_API_KEY")
embeddings = co.embed(texts=["text1", "text2"]).embeddings
```

**Pros:**
- Excellent quality (multilingual)
- Compression support (reduce storage)
- Free tier generous

---

### 3. **Voyage AI** (Code-specific embeddings)
**Speed**: ~40ms per request
**Best for**: Code search

```python
pip install voyageai

import voyageai
vo = voyageai.Client(api_key="YOUR_API_KEY")
embeddings = vo.embed(["def hello()"], model="voyage-code-2")
```

**Pros:**
- Optimized for code/documentation
- Better than sentence-transformers for code
- Supports 32K context

---

### 4. **FastEmbed** (Rust-based, very fast)
**Speed**: 2-3x faster than sentence-transformers on CPU

```bash
pip install fastembed
```

```python
from fastembed import TextEmbedding

embedding_model = TextEmbedding(model_name="BAAI/bge-small-en-v1.5")
embeddings = list(embedding_model.embed(["text1", "text2"]))
```

**Pros:**
- Rust implementation (faster than Python)
- Qdrant's official recommendation
- Drop-in replacement for sentence-transformers

**Cons:**
- Fewer model options
- Less mature ecosystem

---

## Recommended Setup for Your Use Case

### **For Code/Documentation Search (Your Use Case):**

**Option 1: Balanced (Recommended)**
```python
# ONNX + all-MiniLM-L6-v2
embedder = ONNXEmbedder("all-MiniLM-L6-v2", batch_size=64)
# 2-4x faster than baseline, excellent quality
```

**Option 2: Maximum Speed**
```python
# FastEmbed + smaller model
from fastembed import TextEmbedding
embedder = TextEmbedding("BAAI/bge-small-en-v1.5")
# 3-5x faster, good quality
```

**Option 3: Best Quality**
```python
# Voyage AI API (code-specific)
# Or Cohere Embed v3
# Higher quality for code, but costs money
```

---

## Performance Comparison (1000 documents)

| Strategy | Time | Memory | Quality | Cost |
|----------|------|--------|---------|------|
| **Baseline** (sentence-transformers) | 10s | 500MB | ‚≠ê‚≠ê‚≠ê‚≠ê | Free |
| **ONNX** | 3s ‚ö° | 350MB | ‚≠ê‚≠ê‚≠ê‚≠ê | Free |
| **Quantized** | 4s | 125MB üíæ | ‚≠ê‚≠ê‚≠ê‚≠ê | Free |
| **Multi-Process (4 cores)** | 2.5s ‚ö°‚ö° | 1GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Free |
| **FastEmbed** | 3s | 300MB | ‚≠ê‚≠ê‚≠ê‚≠ê | Free |
| **Jina AI API** | 50s (network) | 50MB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $$ |
| **Cohere Embed** | 30s | 50MB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $$ |

---

## Quick Start

### Enable ONNX Optimization (Recommended)
```bash
# 1. Install dependencies
pip install optimum[onnxruntime] onnxruntime

# 2. Set environment variable
echo "EMBEDDING_OPTIMIZATION=onnx" >> .env

# 3. Use converter (auto-detects optimization)
from src.converter import DocumentConverter

converter = DocumentConverter(
    embedding_optimization="onnx"  # 2-4x faster!
)
```

### Enable Multi-Processing
```bash
echo "EMBEDDING_OPTIMIZATION=multiprocess" >> .env
echo "EMBEDDING_WORKERS=4" >> .env  # Use all 4 CPU cores
```

---

## Next Steps

1. **Try ONNX** (easiest, biggest gain)
2. **Try FastEmbed** (if you want even more speed)
3. **Consider API** (if quality > speed)

Want me to implement any of these optimizations?
