{
  "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
  "title": "Streaming",
  "num_chunks": 3,
  "total_chars": 1773,
  "chunks": [
    {
      "index": 0,
      "text": "On this page\n- [Streaming](\\docs\\streaming#streaming)\n- [Enabling streaming](\\docs\\streaming#enabling-streaming)\nReferences\n[TypeScript SDK](\\docs\\reference\\typescript)\n[Serve](\\docs\\reference\\serve)",
      "char_count": 199,
      "token_count": 59,
      "metadata": {
        "title": "Streaming",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "file_name": "streaming.md",
        "file_size": 1861,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "686a6ae01e0c2bebddf02da7e863b22d38972b391edb3010fa089c84876f84be",
        "author": null,
        "created_at": "2025-10-13T00:01:44.824201",
        "modified_at": "2025-10-13T00:01:44.824705",
        "page_count": null,
        "word_count": 230,
        "extracted_at": "2025-10-13T20:05:13.105829",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 3,
        "token_count": 59,
        "has_context": true
      }
    },
    {
      "index": 1,
      "text": "Streaming\nIn select environments, the SDK allows streaming responses back to Inngest, hugely increasing maximum timeouts on many serverless platforms up to 15 minutes.\nWhile we add wider support for streaming to other platforms, we currently support the following:\n- [Cloudflare Workers](\\docs\\learn\\serving-inngest-functions#framework-cloudflare-workers)\n- [Express](\\docs\\learn\\serving-inngest-functions#framework-express)\n- [Next.js on Vercel Fluid Compute or Edge Functions](\\docs\\learn\\serving-inngest-functions#framework-next-js)\n- [Remix on Vercel Edge Functions](\\docs\\learn\\serving-inngest-functions#framework-remix)",
      "char_count": 625,
      "token_count": 147,
      "metadata": {
        "title": "Streaming",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "file_name": "streaming.md",
        "file_size": 1861,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "686a6ae01e0c2bebddf02da7e863b22d38972b391edb3010fa089c84876f84be",
        "author": null,
        "created_at": "2025-10-13T00:01:44.824201",
        "modified_at": "2025-10-13T00:01:44.824705",
        "page_count": null,
        "word_count": 230,
        "extracted_at": "2025-10-13T20:05:13.105829",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 3,
        "token_count": 147,
        "has_context": true
      }
    },
    {
      "index": 2,
      "text": "Streaming\nEnabling streaming\nSelect your platform above and follow the relevant \"Streaming\" section to enable streaming for your application.\nEvery Inngest serve handler provides a\n```\nstreaming\n```\noption, for example:\nCopy Copied\n```\nserve ({\nclient : inngest ,\nfunctions : [ ... fns] ,\nstreaming : \"allow\" ,\n});\n```\nThis can be one of the following values:\n- `false` - Streaming will never be used. This is the default.\n- `\"allow\"` - Streaming will be used if we can confidently detect support for it by verifying that the platform, environment, and serve handler support streaming.\n⚠️ We also allow\n```\n\"force\"\n```\n, where streaming will be used if the serve handler supports it, but completely overrides the SDK's attempts to verify if the platform supports streaming.\nThis is not recommended, but is an escape hatch if you know that streaming is supported and you're in a restricted environment that has little or no access to the environment.",
      "char_count": 949,
      "token_count": 209,
      "metadata": {
        "title": "Streaming",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\streaming.md",
        "file_name": "streaming.md",
        "file_size": 1861,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "686a6ae01e0c2bebddf02da7e863b22d38972b391edb3010fa089c84876f84be",
        "author": null,
        "created_at": "2025-10-13T00:01:44.824201",
        "modified_at": "2025-10-13T00:01:44.824705",
        "page_count": null,
        "word_count": 230,
        "extracted_at": "2025-10-13T20:05:13.105829",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 3,
        "token_count": 209,
        "has_context": true
      }
    }
  ]
}