{
  "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
  "title": "#### On this page",
  "num_chunks": 14,
  "total_chars": 5987,
  "chunks": [
    {
      "index": 0,
      "text": "On this page\n- [AI Agents and RAG](\\docs\\examples\\ai-agents-and-rag#ai-agents-and-rag)\n- [Quick Snippet](\\docs\\examples\\ai-agents-and-rag#quick-snippet)\n- [App examples](\\docs\\examples\\ai-agents-and-rag#app-examples)\n- [Resources](\\docs\\examples\\ai-agents-and-rag#resources)\n- [Related concepts](\\docs\\examples\\ai-agents-and-rag#related-concepts)\n[Examples](\\docs\\examples)",
      "char_count": 373,
      "token_count": 121,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 121,
        "has_context": true
      }
    },
    {
      "index": 1,
      "text": "AI Agents and RAG\nInngest offers tools to support the development of AI-powered applications. Whether you're building AI agents, automating tasks, or orchestrating and managing AI workflows, Inngest provides features that accommodate various needs and requirements, such as concurrency, debouncing, or throttling (see\n[\"Related Concepts\"](\\docs\\examples\\ai-agents-and-rag#related-concepts)\n).",
      "char_count": 392,
      "token_count": 85,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 85,
        "has_context": true
      }
    },
    {
      "index": 2,
      "text": "AI Agents and RAG\nQuick Snippet\nBelow is an example of a RAG workflow (from this\n[example app](https://github.com/inngest/inngest-demo-app/)\n). This asynchronous Inngest function summarizes content via GPT-4 by following these steps:\n- Query a vector database for relevant content.\n- Retrieve a transcript from an S3 file.\n- Combine the transcript and queried content to generate a summary using GPT-4.\n- Save the summary to a database and sends a notification to the client.\nThe function uses\n[Inngest steps](\\docs\\learn\\inngest-steps)\nto guarantee automatic retries on failure.",
      "char_count": 579,
      "token_count": 138,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 138,
        "has_context": true
      }
    },
    {
      "index": 3,
      "text": "AI Agents and RAG\nQuick Snippet\n./inngest/functions.ts\nCopy Copied\n```\nexport const summarizeContent = inngest .createFunction (\n{ name : 'Summarize content via GPT-4' , id : 'summarize-content' } ,\n{ event : 'ai/summarize.content' } ,\nasync ({ event , step , attempt }) => {\nconst results = await step .run ( 'query-vectordb' , async () => {\nreturn {\nmatches : [\n{\nid : 'vec3' ,\nscore : 0 ,\nvalues : [ 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ] ,\ntext : casual .sentences ( 3 ) ,\n} ,\n{\nid : 'vec4' ,\nscore : 0.0799999237 ,\nvalues : [ 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 , 0.4 ] ,\ntext : casual .sentences ( 3 ) ,\n} ,\n{\nid : 'vec2' ,\nscore : 0.0800000429 ,\nvalues : [ 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 ] ,\ntext : casual .sentences ( 3 ) ,\n} ,\n] ,\nnamespace : 'ns1' ,\nusage : { readUnits : 6 } ,\n};\n});\n\nconst transcript = await step .run ( 'read-s3-file' , async () => {\nreturn casual .sentences ( 10 );\n});\n\n// We can globally share throttle limited functions like this using invoke\nconst completion = await step .invoke ( 'generate-summary-via-gpt-4' , {\nfunction : chatCompletion ,\ndata : {\nmessages : [\n{\nrole : 'system' ,\ncontent :\n'You are a helpful assistant that summaries content for product launches.' ,\n} ,\n{\nrole : 'user' ,\ncontent : `Question: Summarize my content: \\n ${ transcript } . \\nInformation: ${ results .matches\n.map ((m) => m .text)\n.join ( '. ' ) } ` ,\n} ,\n] ,\n} ,\n});\n// You might use the response like this:\nconst summary = completion .choices[ 0 ]. message .content;\n\nawait step .run ( 'save-to-db' , async () => {\nreturn casual .uuid;\n});\n\nawait step .run ( 'websocket-push-to-client' , async () => {\nreturn casual .uuid;\n});\nreturn { success : true , summaryId : casual .uuid };\n}\n);\n```",
      "char_count": 1738,
      "token_count": 599,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 599,
        "has_context": true
      }
    },
    {
      "index": 4,
      "text": "AI Agents and RAG\nApp examples\nHere are apps that use Inngest to power AI workflows:",
      "char_count": 84,
      "token_count": 22,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 22,
        "has_context": true
      }
    },
    {
      "index": 5,
      "text": "AI Agents and RAG\nApp examples\nIntegrate AI agents with Inngest\nAI-powered task automation in Next.js using OpenAI and Inngest. Enhance productivity with automated workflows.\nTechnology used : Next.js, OpenAI Explore :\n[Code](https://github.com/joelhooks/inngest-partykit-nextjs-openai)",
      "char_count": 286,
      "token_count": 71,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 71,
        "has_context": true
      }
    },
    {
      "index": 6,
      "text": "AI Agents and RAG\nApp examples\nPCXI starter\nA boilerplate project for the PCXI stack featuring an OpenAI call\nTechnology used : Next.js, OpenAI, Xata, Prisma, Clerk Explore :\n[Code](https://github.com/inngest/next-pxci-starter)\nMade by : Inngest Team",
      "char_count": 250,
      "token_count": 71,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 71,
        "has_context": true
      }
    },
    {
      "index": 7,
      "text": "AI Agents and RAG\nResources\nCheck the resources below to learn more about working with AI using Inngest:",
      "char_count": 104,
      "token_count": 24,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 24,
        "has_context": true
      }
    },
    {
      "index": 8,
      "text": "AI Agents and RAG\nResources\nBlog: \"AI in production: Managing capacity with flow control\"\n[Learn how to manage AI capacity in production using Inngest's flow control techniques, including throttling, concurrency, debouncing, and prioritization, to optimize performance and cost-efficiency.](\\blog\\ai-in-production-managing-capacity-with-flow-control)",
      "char_count": 350,
      "token_count": 75,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 75,
        "has_context": true
      }
    },
    {
      "index": 9,
      "text": "AI Agents and RAG\nResources\nPodcast: \"Building Production Workflows for AI Applications\"\n[Tony Holdstock-Brown and Yoko Li discuss the reality and complexity of running AI agents and other multistep AI workflows in production.](https://a16z.com/podcast/building-production-workflows-for-ai-applications/)",
      "char_count": 304,
      "token_count": 70,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 70,
        "has_context": true
      }
    },
    {
      "index": 10,
      "text": "AI Agents and RAG\nResources\nTalk: \"Automate All of Your Customer Interactions with AI in Next.js\"\n[Joel Hooks discusses managing long-running processes like generative AI to automate customer interactions effectively.](https://www.youtube.com/watch?v=EoFI_Bmzb4g)",
      "char_count": 263,
      "token_count": 61,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 61,
        "has_context": true
      }
    },
    {
      "index": 11,
      "text": "AI Agents and RAG\nResources\nBlog: \"Semi-Autonomous AI Agents and Collaborative Multiplayer Asynchronous Workflows\"\n[Build an AI agent that reads from Linear issues, returns relevant issues based on queries, and allows actions on those issues.](\\blog\\semi-autonomous-ai-agents)",
      "char_count": 276,
      "token_count": 65,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 65,
        "has_context": true
      }
    },
    {
      "index": 12,
      "text": "AI Agents and RAG\nResources\nVideo: \"Chaining Prompts The Easy Way - Using Inngest Serverless Jobs with OpenAI\"\n[Doug Silkstone demonstrates how to chain together prompts and get content in next to no time at all.](https://www.youtube.com/watch?v=PCq6DozV-mY)",
      "char_count": 258,
      "token_count": 69,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 69,
        "has_context": true
      }
    },
    {
      "index": 13,
      "text": "AI Agents and RAG\nRelated concepts\n- [Concurrency](\\docs\\guides\\concurrency) : control the number of steps executing code at any one time.\n- [Debouncing](\\docs\\guides\\debounce) : delay function execution until a series of events are no longer received.\n- [Prioritization](\\docs\\guides\\priority) : dynamically execute some runs ahead or behind others based on any data.\n- [Rate limiting](\\docs\\guides\\rate-limiting) : limit on how many function runs can start within a time period.\n- [Steps](\\docs\\reference\\functions\\step-run) : individual tasks within a function that can be executed independently with a guaranteed retrial.\n- [Throttling](\\docs\\guides\\throttling) : specify how many function runs can start within a time period.",
      "char_count": 730,
      "token_count": 181,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\examples_ai-agents-and-rag.md",
        "file_name": "examples_ai-agents-and-rag.md",
        "file_size": 6554,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "d42d3118a1c747a2605033de178b34f88101d6de11f9cfac7f744fb72d36fec7",
        "author": null,
        "created_at": "2025-10-12T23:58:22.485583",
        "modified_at": "2025-10-12T23:58:22.486151",
        "page_count": null,
        "word_count": 853,
        "extracted_at": "2025-10-13T20:03:12.811497",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 14,
        "token_count": 181,
        "has_context": true
      }
    }
  ]
}