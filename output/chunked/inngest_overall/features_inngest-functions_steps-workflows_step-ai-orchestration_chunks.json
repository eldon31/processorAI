{
  "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
  "title": "#### On this page",
  "num_chunks": 9,
  "total_chars": 10805,
  "chunks": [
    {
      "index": 0,
      "text": "On this page\n- [AI Inference](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#ai-inference)\n- [Benefits](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#benefits)\n- [Step tools: step.ai](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#step-tools-step-ai)\n- [step.ai.infer()](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#step-ai-infer)\n- [step.ai.wrap() (TypeScript only)](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#step-ai-wrap-type-script-only)\n- [Supported providers](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#supported-providers)\n- [Limitations](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#limitations)\n- [AgentKit: AI and agent orchestration](\\docs\\features\\inngest-functions\\steps-workflows\\step-ai-orchestration#agent-kit-ai-and-agent-orchestration)\nFeatures\n[Inngest Functions](\\docs\\features\\inngest-functions)\n[Steps & Workflows](\\docs\\features\\inngest-functions\\steps-workflows)",
      "char_count": 1067,
      "token_count": 320,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 320,
        "has_context": true
      }
    },
    {
      "index": 1,
      "text": "AI Inference TypeScript and Python only\nYou can build complex AI workflows and call model providers as steps using two-step methods,\n```\nstep.ai.infer()\n```\nand\n```\nstep.ai.wrap()\n```\n, or our AgentKit SDK.  They work with any model provider, and all offer full AI observability:\n- `step.ai.wrap()` wraps other AI SDKs (OpenAI, Anthropic, and Vercel AI SDK) as a step, augmenting the observability of your Inngest Functions with information such as prompts and tokens used.\n- `step.ai.infer()` offloads the inference request to Inngest's infrastructure, pausing your function execution until the request finishes. This can be a significant cost saver if you deploy to serverless functions\n- [AgentKit](https://agentkit.inngest.com/) allows you to easily create single model calls or agentic workflows.",
      "char_count": 801,
      "token_count": 187,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 187,
        "has_context": true
      }
    },
    {
      "index": 2,
      "text": "AI Inference TypeScript and Python only\nBenefits\nUsing\n```\nstep.ai\n```\nor\n[AgentKit](https://agentkit.inngest.com/)\nallows you to:\n- Automatically monitor AI usage in production to ensure quality output\n- Easily iterate and test prompts in the dev server\n- Track requests and responses from foundational inference providers\n- Track how inference calls work together in multi-step or agentic workflows",
      "char_count": 400,
      "token_count": 85,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 85,
        "has_context": true
      }
    },
    {
      "index": 3,
      "text": "AI Inference TypeScript and Python only\nStep tools: step.ai TypeScript Python v0.5+\nstep.ai.infer()\nUsing\n```\nstep.ai.infer()\n```\nallows you to call any inference provider's endpoints by offloading it to Inngest's infrastructure.\nAll requests and responses are automatically tracked within your workflow traces.\n**Request offloading**\nOn serverless environments, your function is not executing while the request is in progress - which means you don't pay for function execution while waiting for the provider's response.\nOnce the request finishes, your function restarts with the inference result's data.  Inngest never logs or stores your API keys or authentication headers.  Authentication originates from your own functions.\nHere's an example which calls OpenAI:\nTypeScript Python\nCopy Copied\n```\nexport default inngest .createFunction (\n{ id : \"summarize-contents\" } ,\n{ event : \"app/ticket.created\" } ,\nasync ({ event , step }) => {\n\n// This calls your model's chat endpoint, adding AI observability,\n// metrics, datasets, and monitoring to your calls.\nconst response = await step . ai .infer ( \"call-openai\" , {\nmodel : step . ai . models .openai ({ model : \"gpt-4o\" }) ,\n// body is the model request, which is strongly typed depending on the model\nbody : {\nmessages : [{\nrole : \"assistant\" ,\ncontent : \"Write instructions for improving short term memory\" ,\n}] ,\n} ,\n});\n\n// The response is also strongly typed depending on the model.\nreturn response .choices;\n}\n);\n```",
      "char_count": 1475,
      "token_count": 334,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 334,
        "has_context": true
      }
    },
    {
      "index": 4,
      "text": "AI Inference TypeScript and Python only\nPDF processing with Claude Sonnet and step.ai.infer()\n[Use](https://github.com/inngest/inngest-js/tree/main/examples//step-ai/anthropic-claude-pdf-processing/#readme)\n[```\nstep.ai.infer()\n```](https://github.com/inngest/inngest-js/tree/main/examples//step-ai/anthropic-claude-pdf-processing/#readme)\n[to process a PDF with Claude Sonnet.](https://github.com/inngest/inngest-js/tree/main/examples//step-ai/anthropic-claude-pdf-processing/#readme)",
      "char_count": 485,
      "token_count": 133,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 133,
        "has_context": true
      }
    },
    {
      "index": 5,
      "text": "AI Inference TypeScript and Python only\nPDF processing with Claude Sonnet and step.ai.infer()\nstep.ai.wrap() (TypeScript only)\nUsing\n```\nstep.ai.wrap()\n```\nallows you to wrap other TypeScript AI SDKs, treating each inference call as a step.  This allows you to easily convert AI calls to steps with full observability without changing much application-level code:\nVercel AI SDK Anthropic SDK\nCopy Copied\n```\nimport { generateText } from \"ai\"\nimport { openai } from \"@ai-sdk/openai\"\n\nexport default inngest .createFunction (\n{ id : \"summarize-contents\" } ,\n{ event : \"app/ticket.created\" } ,\nasync ({ event , step }) => {\n\n// This calls `generateText` with the given arguments, adding AI observability,\n// metrics, datasets, and monitoring to your calls.\nconst { text } = await step . ai .wrap ( \"using-vercel-ai\" , generateText , {\nmodel : openai ( \"gpt-4-turbo\" ) ,\nprompt : \"What is love?\"\n});\n\n}\n);\n```\nIn this case, instead of calling the SDK directly, you specify the SDK function you want to call and the function's arguments separately within\n```\nstep.ai.wrap()\n```\n.",
      "char_count": 1074,
      "token_count": 272,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 272,
        "has_context": true
      }
    },
    {
      "index": 6,
      "text": "AI Inference TypeScript and Python only\nPDF processing with Claude Sonnet and step.ai.infer()\nSupported providers\nThe list of current providers supported for\n```\nstep.ai.infer()\n```\nis:\n- `openai` , including any OpenAI compatible API such as Perplexity\n- gemini\n- anthropic\n- grok\n- azure-openai",
      "char_count": 296,
      "token_count": 76,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 76,
        "has_context": true
      }
    },
    {
      "index": 7,
      "text": "AI Inference TypeScript and Python only\nPDF processing with Claude Sonnet and step.ai.infer()\nLimitations\n- Streaming responses from providers is coming soon, alongside real-time support with Inngest functions.\n- When using `step.ai.wrap` with sdk clients that require client instance context to be preserved between invocations, currently it's necessary to bind the client call outside the `step.ai.wrap` call like so:\nWrap Anthropic SDK Wrap OpenAI SDK\nCopy Copied\n```\nimport Anthropic from \"@anthropic-ai/sdk\" ;\nconst anthropic = new Anthropic ();\n\nexport const anthropicWrapGenerateText = inngest .createFunction (\n{ id : \"anthropic-wrap-generateText\" } ,\n{ event : \"anthropic/wrap.generate.text\" } ,\nasync ({ event , step }) => {\n//\n// Will fail because anthropic client requires instance context\n// to be preserved across invocations.\nawait step . ai .wrap (\n\"using-anthropic\" ,\nanthropic . messages .create ,\n{\nmodel : \"claude-3-5-sonnet-20241022\" ,\nmax_tokens : 1024 ,\nmessages : [{ role : \"user\" , content : \"Hello, Claude\" }] ,\n} ,\n);\n\n//\n// Will work beccause we bind to preserve instance context\nconst createCompletion = anthropic . messages . create .bind ( anthropic .messages);\nawait step . ai .wrap (\n\"using-anthropic\" ,\ncreateCompletion ,\n{\nmodel : \"claude-3-5-sonnet-20241022\" ,\nmax_tokens : 1024 ,\nmessages : [{ role : \"user\" , content : \"Hello, Claude\" }] ,\n} ,\n);\n} ,\n);\n```\n- When using `step.ai.wrap` , you can edit prompts and rerun steps in the dev server. But, arguments must be JSON serializable.\nCopy Copied\n```\nimport { generateText as vercelGenerateText } from \"ai\" ;\nimport { openai as vercelOpenAI } from \"@ai-sdk/openai\" ;\n\nexport const vercelWrapGenerateText = inngest .createFunction (\n{ id : \"vercel-wrap-generate-text\" } ,\n{ event : \"vercel/wrap.generate.text\" } ,\nasync ({ event , step }) => {\n//\n// Will work but you will not be able to edit the prompt and rerun the step in the dev server.\nawait step . ai .wrap (\n\"vercel-openai-generateText\" ,\nvercelGenerateText ,\n{\nmodel : vercelOpenAI ( \"gpt-4o-mini\" ) ,\nprompt : \"Write a haiku about recursion in programming.\" ,\n} ,\n);\n\n//\n// Will work and you will be able to edit the prompt and rerun the step in the dev server because\n// the arguments to step.ai.wrap are JSON serializable.\nconst args = {\nmodel : \"gpt-4o-mini\" ,\nprompt : \"Write a haiku about recursion in programming.\" ,\n};\n\nconst gen = ({ model , prompt } : { model : string ; prompt : string }) =>\nvercelGenerateText ({\nmodel : vercelOpenAI (model) ,\nprompt ,\n});\n\nawait step . ai .wrap ( \"using-vercel-ai\" , gen , args);\n} ,\n);\n```\n- `step.ai.wrap's` Typescript definition will for the most part infer allowable inputs based on the signature of the wrapped function. However, in some cases where the wrapped function contains complex overloads, such as Vercel's `generateObject` , it may be necessary to type cast.\n*Note*\n: Future version of the Typescript SDK will correctly infer these complex types, but for now we\nrequire type casting to ensure backward compatibility.\nCopy Copied\n```\nimport { generateObject as vercelGenerateObject } from \"ai\" ;\nimport { openai as vercelOpenAI } from \"@ai-sdk/openai\" ;\n\nexport const vercelWrapSchema = inngest .createFunction (\n{ id : \"vercel-wrap-generate-object\" } ,\n{ event : \"vercel/wrap.generate.object\" } ,\nasync ({ event , step }) => {\n//\n// Calling generateObject directly is fine\nawait vercelGenerateObject ({\nmodel : vercelOpenAI ( \"gpt-4o-mini\" ) ,\nschema : z .object ({\nrecipe : z .object ({\nname : z .string () ,\ningredients : z .array (\nz .object ({ name : z .string () , amount : z .string () }) ,\n) ,\nsteps : z .array ( z .string ()) ,\n}) ,\n}) ,\nprompt : \"Generate a lasagna recipe.\" ,\n});\n\n//\n// step.ai.wrap requires type casting\nawait step . ai .wrap (\n\"vercel-openai-generateObject\" ,\nvercelGenerateObject ,\n{\nmodel : vercelOpenAI ( \"gpt-4o-mini\" ) ,\nschema : z .object ({\nrecipe : z .object ({\nname : z .string () ,\ningredients : z .array (\nz .object ({ name : z .string () , amount : z .string () }) ,\n) ,\nsteps : z .array ( z .string ()) ,\n}) ,\n}) ,\nprompt : \"Generate a lasagna recipe.\" ,\n} as any ,\n);\n} ,\n);\n```",
      "char_count": 4130,
      "token_count": 1108,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 1108,
        "has_context": true
      }
    },
    {
      "index": 8,
      "text": "AI Inference TypeScript and Python only\nAgentKit: AI and agent orchestration TypeScript only\nAgentKit is a simple, standardized way to implement model calling - either as individual calls, a complex workflow, or agentic flows.\nHere's an example of a single model call:\nCopy Copied\n```\nimport { Agent , agenticOpenai as openai , createAgent } from \"@inngest/agent-kit\" ;\nexport default inngest .createFunction (\n{ id : \"summarize-contents\" } ,\n{ event : \"app/ticket.created\" } ,\nasync ({ event , step }) => {\n\n// Create a new agent with a system prompt (you can add optional tools, too)\nconst writer = createAgent ({\nname : \"writer\" ,\nsystem : \"You are an expert writer.  You write readable, concise, simple content.\" ,\nmodel : openai ({ model : \"gpt-4o\" , step }) ,\n});\n\n// Run the agent with an input.  This automatically uses steps\n// to call your AI model.\nconst { output } = await writer .run ( \"Write a tweet on how AI works\" );\n}\n);\n```\n[Read the full AgentKit docs here](https://agentkit.inngest.com/)\nand\n[see the code on GitHub](https://github.com/inngest/agent-kit)\n.",
      "char_count": 1077,
      "token_count": 280,
      "metadata": {
        "title": "#### On this page",
        "source": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "chunk_method": "hybrid",
        "file_path": "C:\\Users\\raze0\\Documents\\LLM_KNOWLEDGE_CREATOR\\RAG\\Docs\\inngest_overall\\features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_name": "features_inngest-functions_steps-workflows_step-ai-orchestration.md",
        "file_size": 11471,
        "file_format": ".md",
        "mime_type": "text/markdown",
        "sha256_hash": "3d6481055fece64249432fcc42fe7532f13cd62bf95d5d4aeb056d6d301b05b5",
        "author": null,
        "created_at": "2025-10-12T23:58:56.131706",
        "modified_at": "2025-10-12T23:58:56.131706",
        "page_count": null,
        "word_count": 1532,
        "extracted_at": "2025-10-13T20:03:25.190215",
        "processing_method": "markdown_docling",
        "custom_metadata": {},
        "total_chunks": 9,
        "token_count": 280,
        "has_context": true
      }
    }
  ]
}